{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 4\n",
    "\n",
    "# This is a mini-project assignment that includes only programming questions. You are asked to implement optimization algorithms for ML classification problems. \n",
    "\n",
    "## Marking of this assignment will be based on the correctness of your ML pipeline and efficiency of your code. \n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and to Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested way of loading data to python for the assigment. There are alternatives of course, you can use your preferred way if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ned/Desktop/CS794/A4\n",
      "/home/ned/Desktop/CS794/A4/libsvm-3.24/python/\n"
     ]
    }
   ],
   "source": [
    "# Download the LIBSVM package from here: https://www.csie.ntu.edu.tw/~cjlin/libsvm/#download \n",
    "# If your download is successfull you should have the folder with name: libsvm-3.24.\n",
    "# We will use this package to load datasets. \n",
    "\n",
    "# Enter the downloaded folder libsvm-3.24 through your terminal. \n",
    "# Run make command to compile the package.\n",
    "\n",
    "# Load this auxiliary package.\n",
    "import sys\n",
    "import os\n",
    "# add here your path to the folder libsvm-3.24/python\n",
    "path = os.getcwd()+'/libsvm-3.24/python/'\n",
    "\n",
    "print(os.getcwd())\n",
    "# Add the path to the Python paths so Python can find the module.\n",
    "sys.path.append(path)\n",
    "# sys.path.append(os.getcwd()+'/libsvm-3.24/')\n",
    "# sys.path.append(os.getcwd()+'/libsvm-3.24/python/')\n",
    "print(path)\n",
    "\n",
    "\n",
    "# Load the LIBSVM module.\n",
    "from svmutil import *\n",
    "\n",
    "# Add here your path to the folder libsvm-3.24\n",
    "path = './libsvm-3.24/heart_scale'\n",
    "\n",
    "# Test that it works. This will load the data \"heart_scale\" \n",
    "# and it will store the labels in \"b\" and the data matrix in \"A\".\n",
    "b, A = svm_read_problem(path)\n",
    "\n",
    "# Use \"svm_read_problem\" function to load data for your assignment.\n",
    "\n",
    "# Note that matrix \"A\" stores the data in a sparse format. \n",
    "# In particular matrix \"A\" is a list of dictionaries. \n",
    "# The length of the list gives you the number of samples.\n",
    "# Each entry in the list is a dictionary. The keys of the dictionary are the non-zero features.\n",
    "# The values of the dictionary for each key is a list which gives you the feature value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load other useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# my import \n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "import time\n",
    "from scipy import real, ndimage\n",
    "from scipy.sparse import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets that you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an extended selection of classification and regression datasets \n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
    "\n",
    "# Out of all these datasets you will need the following 3 datasets, which are datasets for classification problems.\n",
    "# \n",
    "# a9a dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a \n",
    "# This dataset is small, it is recommened to start your experiments with this dataset.\n",
    "#\n",
    "# epsilon dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#epsilon \n",
    "# This dataset relatively large, but most importantly it is ill-conditioned. This means that the\n",
    "# methods might stagnate fast.\n",
    "#\n",
    "# kdd2010 (algebra): https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#kdd2010%20(algebra)\n",
    "# This dataset is large and very sparse. Exploit the sparsity of the problem when you implement optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets above consist of training and testing data. \n",
    "\n",
    "# You should seperate the training data into training and validation data.\n",
    "# Follow the instructions from the lectures about how you can use both training and validation data.\n",
    "# You can use 10% of the training data as validation data and the remaining 90% to train the models.\n",
    "# This is a suggested percentage, you can do otherwise if you wish.\n",
    "\n",
    "# Do not use the testing data to influence training in any way. Do not use the testing data at all.\n",
    "# Only your instructor and TA will use the testing data to measure generalization error. \n",
    "# If you do use the testing data to tune parameters or for training of the algorithms we will figure it out :-). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to solve the following optimization problems \n",
    "\n",
    "Hinge-loss\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d, \\beta \\in \\mathbb{R}} \\ \\frac{1}{n} \\sum_{i=1}^n \\max \\{0,1-b_i(a_i^Tx + \\beta)\\},$$\n",
    "where $a_i\\in\\mathbb{R}^d$ is the feature vector for sample $i$ and $b_i$ is the label of sample $i$. The sub-gradient of the hinge-loss is given in the lecture slides (note that there is a small difference due to the intercept $\\beta$). A smooth approximation of the function $f(z):=\\max\\{0,1-z\\}$ is given by\n",
    "$$\n",
    "\\psi_\\mu(z) = \n",
    "\\begin{cases}\n",
    "0 & z\\ge 1\\\\\n",
    "1-z^2 & \\mu < z < 1 \\\\\n",
    "(1-\\mu)^2 + 2(1-\\mu)(\\mu-z) & z \\le \\mu.\n",
    "\\end{cases}\n",
    "$$\n",
    "You can use the smooth approximation $\\psi_\\mu(z)$ for methods that work only for smooth functions. For sub-gradient methods you should use the sub-gradient.\n",
    "\n",
    "L2-regularized logistic regression\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d,\\beta\\in\\mathbb{R}} \\ \\lambda \\|x\\|_2^2 + \\frac{1}{n} \\sum_{i=1}^n \\log (1+ \\exp(-b_i(a_i^Tx + \\beta))).$$\n",
    "This is a smooth objective function, therefore, you should use gradient methods to solve it. You do not need sub-gradient methods for this problem.\n",
    "\n",
    "Note that for hinge-loss the labels should be -1 or 1. For logistic regression the labels should be 0 or 1. Check the labels that you loaded to make sure that they have the correct values for the correct problem. If not, then convert them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this assignment you will need the following methods\n",
    "\n",
    "# 1) Stochastic sub-gradient\n",
    "# 2) Stochastic gradient\n",
    "# 3) Mini-batch (sub-)gradient (you will have to decide what batching strategy to use, see lecture slides)\n",
    "# 4) Stochastic average sub-gradient (SAG)\n",
    "# 5) Stochastic average gradient (SAG)\n",
    "# 6) Gradient descent with Armijo line-search\n",
    "# 7) Acceleratd gradient with Armijo line-search (the same method as Q5 in Assignemnt 3)\n",
    "\n",
    "# Information is provided in the lecture slides about parameter tuning and termination.\n",
    "# However, the final decision of any parameter tuning and termination criteria is up to the students to make. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation error: measure the validation error by calculating\n",
    "$$\n",
    "\\frac{1}{t}\\sum_{i\\in\\mbox{validation data}} \\left| \\ b_i^{\\mbox{your model}} - b_i^{\\mbox{true}} \\ \\right|\n",
    "$$\n",
    "where $t$ is the number of samples in your validation set. $b_i^{\\mbox{true}}$ is the true label of the $i$-th sample. $b_i^{\\mbox{your model}}$ is the label of the $i$-th sample of your model.\n",
    "\n",
    "For hinge loss calculate $$b_i^{\\mbox{your model}}:= \\mbox{sign}(a_i^Tx + \\beta).$$\n",
    "\n",
    "For logistic regression calculate the predicted label by\n",
    "$$\n",
    "b_i^{\\mbox{your model}}=\n",
    "\\begin{cases}\n",
    "1 & \\mbox{if } a_i^Tx + \\beta > 0\\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Use the ML pipeline that is mentioned in slide 60 of Lecture 11 to train your model for the logistic regression problem (the hinge-loss problem does not have any hyper-parameters). Pick any algorithm that you want from the above suggested list to train the models. Report your ML pipeline. Print your Generalization Error. We will not measure running time for this pipeline. Running time will be measure only in Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Plot the objective function (y-axis) vs running time in sec (x-axis). Have one plot for each optimization problem. In each plot show the performance of all relevant algorithms. For each plot use the parameter setting that gives you the best validation error in Q1 (this refers to the logistic regression probelm). Do not show plots for all parameter settings that you tried in Q1, only for the one that gives you the smallest validation error. Do not include computation of any plot data in the computation of the running time of the algorithm, unless the plot data are computed by the algorithm anyway. Make sure that the plots are clean and use appropriate legends. Note that we should be able to re-run the code and obtain the plots. \n",
    "\n",
    "### For this question, we will measure the running time of your stochastic sub-gradient method for the sparse dataset kdd2010 (algebra) for the hinge-loss problem. We will not measure the running time of any other combination of algorithm, dataset, problem. You need to implement the stochastic sub-gradient method and encapsulate it in a python class.\n",
    "\n",
    "To make sure your object can be used by our script, your class should have two methods:\n",
    "\n",
    "1. <strong>fit(self, train_data, train_label)</strong>. It will use stochastic sub-gradient method to minimize the hinge loss and store the optimized coefficients (i.e. $x, \\beta$) in the instance. The \"train_data\" and \"train_label\" are similar to the output of \"svm_read_problem\". \n",
    "    * \"train_data\" is a list of $n$ python dictionaries (int -> float), which presents a sparse matrix. The keys (int) and values (float) in the dictionary at train_data[i] are the indices (int) and values (float) of non-zero entries of row $i$. \n",
    "    * \"train_label\" is a list of $n$ integers, it only has <strong>-1s and 1s</strong>. $n$ is the number of samples.  This function returns nothing.\n",
    "\n",
    "\n",
    "2. <strong>predict(self, test_data)</strong>. It will predict the label of the input \"test_data\" by using the coefficients stored in the instance. The \"test_data\" has the same data structure as the \"train_data\" of the \"fit\" function. This function returns a list of <strong>-1s and 1s</strong> (i.e. the prediction of your labels).\n",
    "\n",
    "You can also define other methods to help your programming, we will only call the two methods decribed above.\n",
    "\n",
    "To let us import your class, you need to follow these rules:\n",
    "\n",
    "1. You should name your python file by <strong>a4_[your student ID].py</strong>. For example, if your student id is 12345, then your file name is <strong>a4_12345.py</strong>\n",
    "1. Your object name should be <strong>MyMethod</strong> (it's case sensitive).\n",
    "\n",
    "Any violation of the above requirements will get error in our script and you will get at most 50% of the total score. Your solution will be mainly measured by the runing time of the <strong>fit</strong> function and the accuracy of the <strong>predict</strong> function. For example your method will be called and measured in following pattern:\n",
    "\n",
    "    obj = MyMethod()\n",
    "    st = time.time()\n",
    "    obj.fit(train_data, train_label) # .fit() optimizes the objective and stores coefficients in obj.\n",
    "    running_time = time.time() - st\n",
    "    predict_label = obj.predict(test_data)\n",
    "    accuracy = get_accuracy(predict_label, test_label) # this is a function we use to measure accuracy.\n",
    "Then your accuracy will be measured by <strong>predict_labels</strong>, you don't have to implement \"get_accuracy\". When you finish your implementation, upload the .py file to Learn dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Q1\n",
    "########################################################################\n",
    "\n",
    "# Read from a9a\n",
    "from scipy.sparse import *\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# A is a list of Dictionary, b is a list of int. \n",
    "b,A = svm_read_problem(os.getcwd()+'/a9a')\n",
    "# 90% training and 10%testing\n",
    "A_training = A[:int(0.9*len(A))]\n",
    "b_training = b[:int(0.9*len(b))]\n",
    "A_testing = A[int(0.9*len(A)):]\n",
    "b_testing = b[int(0.9*len(b)):]\n",
    "\n",
    "\n",
    "# of columns\n",
    "features = 123\n",
    "print(features)\n",
    "\n",
    "# A_matrix is scipy.sparse.csr_matrix, \n",
    "vec = DictVectorizer()\n",
    "A_matrix = vec.fit_transform(A_training)\n",
    "\n",
    "b_column = np.array(b_training).reshape(len(b_training),1)\n",
    "x = csr_matrix(np.ones(shape=(features,1), dtype=float), shape=(features,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 1)\n",
      "stochastic_subgradient\n",
      "(<123x1 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 123 stored elements in Compressed Sparse Row format>, 407, 13359, 0.49527455732598585)\n",
      "0.49527455732598585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# help functions\n",
    "# A_matrix is scipy.sparse.csr_matrix, x is scipy.sparse.csr.csr_matrix, b is numpy.ndarray \n",
    "import random,math\n",
    "def get_hinge_loss(A,x,b):\n",
    "#     print('get_hinge_loss')\n",
    "    A_dot_x = A.dot(x)\n",
    "    ei = A_dot_x.multiply(b).tocsr()  # scipy.sparse.coo.coo_matrix ->csr a matrix \n",
    "    one_error = lambda ei : max(0,1-ei)\n",
    "    total_error = sum(map(one_error,[ei[i,0] for i in range(ei.shape[0])]))\n",
    "    return total_error/A.shape[0]\n",
    "    \n",
    "# return a csr_matrix (row vector)\n",
    "def get_gradient(A,x,b,i):\n",
    "#     print('get_gradient')\n",
    "    if 1 - (b[i]*(A[i].dot(x))[0,0]) > 0:\n",
    "        grad = (-b[i]*A[i]).reshape(features,1)\n",
    "#         print(type(grad),grad.shape)\n",
    "        return grad\n",
    "    else: \n",
    "        grad = np.zeros(shape=(features,1), dtype=float)\n",
    "#         print(type(grad),grad.shape)\n",
    "        return grad\n",
    "        \n",
    "        \n",
    "def stochastic_subgradient(A,x,b,max_iteration,epsilon):\n",
    "    print(x.shape)\n",
    "    count = 2\n",
    "    print('stochastic_subgradient')\n",
    "    hinge_loss = epsilon # initial a size to start \n",
    "    while count < max_iteration and hinge_loss >= epsilon:\n",
    "        index = random.randint(0,len(b))\n",
    "        hinge_loss = get_hinge_loss(A,x,b)\n",
    "        grad = get_gradient(A,x,b,index)\n",
    "        step_size = 1/math.log(count,2)   \n",
    "#         print('count %d\\t i %d\\t stepsize %.4f \\t hinge_loss %.4f\\t sum_of_grad %d'%(count,index, step_size,hinge_loss, sum(grad)))\n",
    "        # stepsize = 1/couont\n",
    "  \n",
    "        \n",
    "        x = x - step_size*csr_matrix(grad,shape=(grad.shape[0],1))\n",
    "        count +=1\n",
    "    return (x,count,index,hinge_loss)\n",
    "solution = stochastic_subgradient(A_matrix,x,b_column, 1000,0.5)\n",
    "print(solution)\n",
    "print(solution[3])\n",
    "\n",
    "\n",
    "\n",
    "# def get_hinge_loss(a,x,b):\n",
    "#     term = 1-b*a.dot(x)\n",
    "#     term = np.where(term>0, term , 0)\n",
    "#     result = (1/b.shape[0])*np.sum(term)\n",
    "#     return result\n",
    "\n",
    "# def get_gradient(x,bi,ai):\n",
    "#     term = bi*ai.dot(x)\n",
    "#     if 1-term>0:\n",
    "#         result = -bi*ai\n",
    "#         result = np.reshape(result,(len(result),1))\n",
    "#     else:\n",
    "#         result = np.zeros((x.shape[0],1))\n",
    "#     return result\n",
    "\n",
    "# def stochastic_subgradient(A,x,b,max_iteration,epsilon):\n",
    "#     print(x.shape)\n",
    "#     count = 0\n",
    "#     print('stochastic_subgradient')\n",
    "#     hinge_loss = epsilon # initial a size to start \n",
    "#     while count < max_iteration and hinge_loss >= epsilon:\n",
    "#         index = random.randint(0,len(b))\n",
    "#         hinge_loss = get_hinge_loss(A,x,b)\n",
    "#         grad = get_gradient(A,x,b,index)\n",
    "#         print('count %d\\t random_i %d\\t hinge_loss %.4f\\t sum_of_grad %d'%(count,index,hinge_loss, sum(grad)))\n",
    "#         # stepsize = 1/couont\n",
    "#         step_size = 1/(count+1)        \n",
    "#         if sum(grad) != 0:\n",
    "#             x = x - step_size*csr_matrix(grad,shape=(grad.shape[0],1))\n",
    "#             count +=1\n",
    "        \n",
    "# stochastic_subgradient(A_matrix,x,b_column, 2000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question2:\n",
    "class MyMethod(object):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
