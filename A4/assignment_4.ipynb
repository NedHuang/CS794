{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 4\n",
    "\n",
    "# This is a mini-project assignment that includes only programming questions. You are asked to implement optimization algorithms for ML classification problems. \n",
    "\n",
    "## Marking of this assignment will be based on the correctness of your ML pipeline and efficiency of your code. \n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and to Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested way of loading data to python for the assigment. There are alternatives of course, you can use your preferred way if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/DaHuang/Desktop/CS794/A4\n",
      "/Users/DaHuang/Desktop/CS794/A4/libsvm-3.24/python/\n"
     ]
    }
   ],
   "source": [
    "# Download the LIBSVM package from here: https://www.csie.ntu.edu.tw/~cjlin/libsvm/#download \n",
    "# If your download is successfull you should have the folder with name: libsvm-3.24.\n",
    "# We will use this package to load datasets. \n",
    "\n",
    "# Enter the downloaded folder libsvm-3.24 through your terminal. \n",
    "# Run make command to compile the package.\n",
    "\n",
    "# Load this auxiliary package.\n",
    "import sys\n",
    "import os\n",
    "# add here your path to the folder libsvm-3.24/python\n",
    "path = os.getcwd()+'/libsvm-3.24/python/'\n",
    "\n",
    "print(os.getcwd())\n",
    "# Add the path to the Python paths so Python can find the module.\n",
    "sys.path.append(path)\n",
    "# sys.path.append(os.getcwd()+'/libsvm-3.24/')\n",
    "# sys.path.append(os.getcwd()+'/libsvm-3.24/python/')\n",
    "print(path)\n",
    "\n",
    "\n",
    "# Load the LIBSVM module.\n",
    "from svmutil import *\n",
    "\n",
    "# Add here your path to the folder libsvm-3.24\n",
    "path = './libsvm-3.24/heart_scale'\n",
    "\n",
    "# Test that it works. This will load the data \"heart_scale\" \n",
    "# and it will store the labels in \"b\" and the data matrix in \"A\".\n",
    "b, A = svm_read_problem(path)\n",
    "\n",
    "# Use \"svm_read_problem\" function to load data for your assignment.\n",
    "\n",
    "# Note that matrix \"A\" stores the data in a sparse format. \n",
    "# In particular matrix \"A\" is a list of dictionaries. \n",
    "# The length of the list gives you the number of samples.\n",
    "# Each entry in the list is a dictionary. The keys of the dictionary are the non-zero features.\n",
    "# The values of the dictionary for each key is a list which gives you the feature value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load other useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# my import \n",
    "from numpy.linalg import norm\n",
    "import math,random,time, random\n",
    "from scipy import real, ndimage\n",
    "from scipy.sparse import *\n",
    "\n",
    "from scipy.sparse import *\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from  scipy.sparse.linalg import expm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets that you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an extended selection of classification and regression datasets \n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
    "\n",
    "# Out of all these datasets you will need the following 3 datasets, which are datasets for classification problems.\n",
    "# \n",
    "# a9a dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a \n",
    "# This dataset is small, it is recommened to start your experiments with this dataset.\n",
    "#\n",
    "# epsilon dataset: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#epsilon \n",
    "# This dataset relatively large, but most importantly it is ill-conditioned. This means that the\n",
    "# methods might stagnate fast.\n",
    "#\n",
    "# kdd2010 (algebra): https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#kdd2010%20(algebra)\n",
    "# This dataset is large and very sparse. Exploit the sparsity of the problem when you implement optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets above consist of training and testing data. \n",
    "\n",
    "# You should seperate the training data into training and validation data.\n",
    "# Follow the instructions from the lectures about how you can use both training and validation data.\n",
    "# You can use 10% of the training data as validation data and the remaining 90% to train the models.\n",
    "# This is a suggested percentage, you can do otherwise if you wish.\n",
    "\n",
    "# Do not use the testing data to influence training in any way. Do not use the testing data at all.\n",
    "# Only your instructor and TA will use the testing data to measure generalization error. \n",
    "# If you do use the testing data to tune parameters or for training of the algorithms we will figure it out :-). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to solve the following optimization problems \n",
    "\n",
    "Hinge-loss\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d, \\beta \\in \\mathbb{R}} \\ \\frac{1}{n} \\sum_{i=1}^n \\max \\{0,1-b_i(a_i^Tx + \\beta)\\},$$\n",
    "where $a_i\\in\\mathbb{R}^d$ is the feature vector for sample $i$ and $b_i$ is the label of sample $i$. The sub-gradient of the hinge-loss is given in the lecture slides (note that there is a small difference due to the intercept $\\beta$). A smooth approximation of the function $f(z):=\\max\\{0,1-z\\}$ is given by\n",
    "$$\n",
    "\\psi_\\mu(z) = \n",
    "\\begin{cases}\n",
    "0 & z\\ge 1\\\\\n",
    "(1-z)^2 & \\mu < z < 1 \\\\\n",
    "(1-\\mu)^2 + 2(1-\\mu)(\\mu-z) & z \\le \\mu.\n",
    "\\end{cases}\n",
    "$$\n",
    "You can use the smooth approximation $\\psi_\\mu(z)$ for methods that work only for smooth functions. For sub-gradient methods you should use the sub-gradient.\n",
    "\n",
    "L2-regularized logistic regression\n",
    "$$\\mbox{minimize}_{x\\in\\mathbb{R}^d,\\beta\\in\\mathbb{R}} \\ \\lambda \\|x\\|_2^2 + \\frac{1}{n} \\sum_{i=1}^n \\log (1+ \\exp(-b_i(a_i^Tx + \\beta))).$$\n",
    "This is a smooth objective function, therefore, you should use gradient methods to solve it. You do not need sub-gradient methods for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this assignment you will need the following methods\n",
    "\n",
    "# 1) Stochastic sub-gradient\n",
    "# 2) Stochastic gradient\n",
    "# 3) Mini-batch (sub-)gradient (you will have to decide what batching strategy to use, see lecture slides)\n",
    "# 4) Stochastic average sub-gradient (SAG)\n",
    "# 5) Stochastic average gradient (SAG)\n",
    "# 6) Gradient descent with Armijo line-search\n",
    "# 7) Acceleratd gradient with Armijo line-search (the same method as Q5 in Assignemnt 3)\n",
    "\n",
    "# Information is provided in the lecture slides about parameter tuning and termination.\n",
    "# However, the final decision of any parameter tuning and termination criteria is up to the students to make. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation error: measure the validation error by calculating\n",
    "$$\n",
    "\\frac{1}{t}\\sum_{i\\in\\mbox{validation data}} \\left| \\ b_i^{\\mbox{your model}} - b_i^{\\mbox{true}} \\ \\right|\n",
    "$$\n",
    "where $t$ is the number of samples in your validation set. $b_i^{\\mbox{true}}$ is the true label of the $i$-th sample. $b_i^{\\mbox{your model}}$ is the label of the $i$-th sample of your model.\n",
    "\n",
    "For hinge loss calculate $$b_i^{\\mbox{your model}}:= \\mbox{sign}(a_i^Tx + \\beta).$$\n",
    "\n",
    "For logistic regression calculate the predicted label by\n",
    "$$\n",
    "b_i^{\\mbox{your model}}=\n",
    "\\begin{cases}\n",
    "1 & \\mbox{if } \\frac{1}{1+e^{-(a_i^Tx + \\beta)}} > 0.5\\\\\n",
    "-1 & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Use the ML pipeline that is mentioned in slide 60 of Lecture 11 to train your model for the logistic regression problem (the hinge-loss problem does not have any hyper-parameters). Pick any algorithm that you want from the above suggested list to train the models. Report your ML pipeline. Print your Generalization Error. We will not measure running time for this pipeline. Running time will be measure only in Q2. Marks: 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Plot the objective function (y-axis) vs running time in sec (x-axis). Have one plot for each optimization problem. In each plot show the performance of all relevant algorithms. For each plot use the parameter setting that gives you the best validation error in Q1 (this refers to the logistic regression probelm). Do not show plots for all parameter settings that you tried in Q1, only for the one that gives you the smallest validation error. Do not include computation of any plot data in the computation of the running time of the algorithm, unless the plot data are computed by the algorithm anyway. Make sure that the plots are clean and use appropriate legends. Note that we should be able to re-run the code and obtain the plots. Marks: 70.\n",
    "\n",
    "### For this question, we will measure the running time of your stochastic sub-gradient method for the sparse dataset news20.binary for the hinge-loss problem. We will not measure the running time of any other combination of algorithm, dataset, problem. You need to implement the stochastic sub-gradient method and encapsulate it in a python class.\n",
    "\n",
    "To make sure your object can be used by our script, your class should have two methods:\n",
    "\n",
    "1. <strong>fit(self, train_data, train_label)</strong>. It will use stochastic sub-gradient method to minimize the hinge loss and store the optimized coefficients (i.e. $x, \\beta$) in the instance. The \"train_data\" and \"train_label\" are similar to the output of \"svm_read_problem\". \n",
    "    * \"train_data\" is a list of $n$ python dictionaries (int -> float), which presents a sparse matrix. The keys (int) and values (float) in the dictionary at train_data[i] are the indices (int) and values (float) of non-zero entries of row $i$. \n",
    "    * \"train_label\" is a list of $n$ integers, it only has <strong>-1s and 1s</strong>. $n$ is the number of samples.  This function returns nothing.\n",
    "\n",
    "\n",
    "2. <strong>predict(self, test_data)</strong>. It will predict the label of the input \"test_data\" by using the coefficients stored in the instance. The \"test_data\" has the same data structure as the \"train_data\" of the \"fit\" function. This function returns a list of <strong>-1s and 1s</strong> (i.e. the prediction of your labels).\n",
    "\n",
    "You can also define other methods to help your programming, we will only call the two methods decribed above.\n",
    "\n",
    "To let us import your class, you need to follow these rules:\n",
    "\n",
    "1. You should name your python file by <strong>a4_[your student ID].py</strong>. For example, if your student id is 12345, then your file name is <strong>a4_12345.py</strong>\n",
    "1. Your object name should be <strong>MyMethod</strong> (it's case sensitive).\n",
    "\n",
    "Any violation of the above requirements will get error in our script and you will get at most 50% of the total score. Your solution will be mainly measured by the runing time of the <strong>fit</strong> function and the accuracy of the <strong>predict</strong> function. For example your method will be called and measured in following pattern:\n",
    "\n",
    "    obj = MyMethod()\n",
    "    st = time.time()\n",
    "    obj.fit(train_data, train_label) # .fit() optimizes the objective and stores coefficients in obj.\n",
    "    running_time = time.time() - st\n",
    "    predict_label = obj.predict(test_data)\n",
    "    accuracy = get_accuracy(predict_label, test_label) # this is a function we use to measure accuracy.\n",
    "Then your accuracy will be measured by <strong>predict_labels</strong>, you don't have to implement \"get_accuracy\". When you finish your implementation, upload the .py file to Learn dropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Q1\n",
    "########################################################################\n",
    "\n",
    "# Read from a9a\n",
    "\n",
    "# A is a list of Dictionary, b is a list of int. \n",
    "b,A = svm_read_problem(os.getcwd()+'/a9a')\n",
    "features = 123\n",
    "vec = DictVectorizer()\n",
    "A_matrix = A_matrix = vec.fit_transform(A).tocsr()\n",
    "b_matrix = csr_matrix(np.array(b).reshape(len(b),1), shape=(len(b),1))\n",
    "\n",
    "# shuffle matrix\n",
    "from sklearn.utils import shuffle\n",
    "A_shuffled, b_shuffled = shuffle(A_matrix,b_matrix)\n",
    "\n",
    "# Ax+beta = b, adding one column of one to A and append beta to x\n",
    "A_ = hstack((A_shuffled, csr_matrix(np.ones(shape=(A_shuffled.shape[0],1), dtype=float), shape=(len(b),1)))).tocsr()\n",
    "x_ = csr_matrix(np.ones(shape=(features+1,1), dtype=float), shape=(features+1,1)) # features +1 = [x+beta]\n",
    "b_ = b_shuffled\n",
    "\n",
    "# 90% training and 10% testing\n",
    "A_training = A_[:int(0.9*A_.shape[0])]\n",
    "b_training = b_[:int(0.9*b_.shape[0])]\n",
    "A_testing = A_[int(0.9*A_.shape[0]):]\n",
    "b_testing = b_[int(0.9*b_.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# help functions\n",
    "# A_matrix is scipy.sparse.csr_matrix, x is scipy.sparse.csr.csr_matrix, b is numpy.ndarray \n",
    "import random\n",
    "lambda_ = 1\n",
    "def get_hinge_loss(A,x,b):\n",
    "    A_dot_x = A.dot(x)\n",
    "    ei = A_dot_x.multiply(b).tocsr()  # scipy.sparse.coo.coo_matrix ->csr a matrix \n",
    "    one_error = lambda ei : max(0,1-ei)\n",
    "    total_error = sum(map(one_error,[ei[i,0] for i in range(ei.shape[0])]))\n",
    "    return total_error/A.shape[0]\n",
    "\n",
    "# get_hinge_loss(A_,x_,b_)\n",
    "    \n",
    "    \n",
    "# return a csr_matrix (row vector)\n",
    "def get_gradient(A,x,b,i):\n",
    "#     print('get_gradient')\n",
    "    if 1 - (b[i]*(A[i].dot(x))).tocsr()[0,0] > 0:\n",
    "        grad = (-b[i]*A[i]).reshape(features+1,1).tocsr()\n",
    "        return grad\n",
    "    else: \n",
    "        grad = csr_matrix(np.zeros(shape=(features+1,1), dtype=float))\n",
    "        return grad\n",
    "# get_gradient(A_,x_,b_,10)        \n",
    "        \n",
    "def stochastic_subgradient(A,x,b,max_iteration,epsilon):\n",
    "    count = 2 # decrease by 1/log(count), denominator cant be zero\n",
    "    print('stochastic_subgradient')\n",
    "    hinge_loss = epsilon # initial a size to start \n",
    "    while count < max_iteration and hinge_loss >= epsilon:\n",
    "        index = random.randint(0,b.shape[0])\n",
    "        hinge_loss = get_hinge_loss(A,x,b)\n",
    "        grad = get_gradient(A,x,b,index)\n",
    "        step_size = 1/math.log(count,2) \n",
    "#         print('count %d\\t i %d\\t stepsize %.4f \\t hinge_loss %.4f\\t sum_of_grad %d'%(count,index, step_size,hinge_loss, grad.sum()))\n",
    "        stepsize = 1/math.log(count)\n",
    "        x = x - step_size*grad\n",
    "        count +=1\n",
    "    return x,count,hinge_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic_subgradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d2ea887b326e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_sgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_sgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhinge_loss_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_subgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_sgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhinge_loss_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fefd5e695773>\u001b[0m in \u001b[0;36mstochastic_subgradient\u001b[0;34m(A, x, b, max_iteration, epsilon)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iteration\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhinge_loss\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhinge_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hinge_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fefd5e695773>\u001b[0m in \u001b[0;36mget_hinge_loss\u001b[0;34m(A, x, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_dot_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scipy.sparse.coo.coo_matrix ->csr a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mone_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mei\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtotal_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_error\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fefd5e695773>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_dot_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scipy.sparse.coo.coo_matrix ->csr a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mone_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mei\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtotal_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_error\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_get_intXint\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    644\u001b[0m         indptr, indices, data = get_csr_submatrix(\n\u001b[1;32m    645\u001b[0m             \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             major, major + 1, minor, minor + 1)\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Q1 training\n",
    "########################################################################\n",
    "\n",
    "x_sgd,count_sgd,hinge_loss_sgd = stochastic_subgradient(A_training,x_,b_training, 1000,0.5)\n",
    "\n",
    "print(count_sgd,hinge_loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-58abf448b655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_dot_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_error_Q1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_testing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_sgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_testing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_sgd' is not defined"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Q1 \n",
    "########################################################################\n",
    "# validation error\n",
    "# print(get_hinge_loss(A_testing,x_sgd,b_testing))\n",
    "\n",
    "def validation_error_Q1(A,x,b):\n",
    "    A_dot_x = A.dot(x).tocsr()\n",
    "    s = A_dot_x.sign()\n",
    "    return np.sum(np.absolute(s-b))/b.shape[0]\n",
    "print(validation_error_Q1(A_testing,x_sgd,b_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic_subgradient, question2\n",
      "1573336705.143268\n",
      "count 2\t i 23798\t stepsize 1.0000 \t hinge_loss 12.0189\t time 0.8955\n",
      "count 3\t i 4190\t stepsize 0.6309 \t hinge_loss 12.0189\t time 1.7760\n",
      "count 4\t i 9680\t stepsize 0.5000 \t hinge_loss 12.0189\t time 2.5946\n",
      "count 5\t i 22304\t stepsize 0.4307 \t hinge_loss 8.9150\t time 3.4085\n",
      "count 6\t i 28464\t stepsize 0.3869 \t hinge_loss 8.9150\t time 4.2397\n",
      "count 7\t i 26939\t stepsize 0.3562 \t hinge_loss 6.8849\t time 5.0423\n",
      "count 8\t i 19652\t stepsize 0.3333 \t hinge_loss 5.2177\t time 5.8494\n",
      "count 9\t i 13009\t stepsize 0.3155 \t hinge_loss 3.6016\t time 6.6667\n",
      "count 10\t i 28432\t stepsize 0.3010 \t hinge_loss 1.8043\t time 7.4667\n",
      "count 11\t i 25732\t stepsize 0.2891 \t hinge_loss 1.8043\t time 8.2671\n",
      "count 12\t i 845\t stepsize 0.2789 \t hinge_loss 0.8696\t time 9.0833\n",
      "count 13\t i 19266\t stepsize 0.2702 \t hinge_loss 0.6558\t time 9.8903\n",
      "count 14\t i 14830\t stepsize 0.2626 \t hinge_loss 0.6558\t time 10.7117\n",
      "count 15\t i 27106\t stepsize 0.2560 \t hinge_loss 0.6558\t time 11.5439\n",
      "count 16\t i 22191\t stepsize 0.2500 \t hinge_loss 0.9840\t time 12.3558\n",
      "count 17\t i 27587\t stepsize 0.2447 \t hinge_loss 0.9840\t time 13.1589\n",
      "count 18\t i 18192\t stepsize 0.2398 \t hinge_loss 0.6489\t time 13.9705\n",
      "count 19\t i 27741\t stepsize 0.2354 \t hinge_loss 1.0909\t time 14.7823\n",
      "count 20\t i 8289\t stepsize 0.2314 \t hinge_loss 0.6447\t time 15.6137\n",
      "count 21\t i 24069\t stepsize 0.2277 \t hinge_loss 0.6447\t time 16.4333\n",
      "count 22\t i 20660\t stepsize 0.2242 \t hinge_loss 0.6447\t time 17.2297\n",
      "count 23\t i 244\t stepsize 0.2211 \t hinge_loss 0.6447\t time 18.0471\n",
      "count 24\t i 23014\t stepsize 0.2181 \t hinge_loss 0.6698\t time 18.8710\n",
      "count 25\t i 14477\t stepsize 0.2153 \t hinge_loss 0.6698\t time 19.7107\n",
      "count 26\t i 16396\t stepsize 0.2127 \t hinge_loss 0.6698\t time 20.5234\n",
      "count 27\t i 17912\t stepsize 0.2103 \t hinge_loss 0.6698\t time 21.3834\n",
      "count 28\t i 4277\t stepsize 0.2080 \t hinge_loss 0.6698\t time 22.2186\n",
      "count 29\t i 4953\t stepsize 0.2058 \t hinge_loss 0.6627\t time 23.0277\n",
      "count 30\t i 17998\t stepsize 0.2038 \t hinge_loss 0.6627\t time 23.8417\n",
      "count 31\t i 16195\t stepsize 0.2018 \t hinge_loss 1.0971\t time 24.6969\n",
      "count 32\t i 4296\t stepsize 0.2000 \t hinge_loss 1.0971\t time 25.5326\n",
      "count 33\t i 13666\t stepsize 0.1982 \t hinge_loss 1.0971\t time 26.3554\n",
      "count 34\t i 13142\t stepsize 0.1966 \t hinge_loss 0.6887\t time 27.1549\n",
      "count 35\t i 4164\t stepsize 0.1950 \t hinge_loss 0.6887\t time 27.9688\n",
      "count 36\t i 4875\t stepsize 0.1934 \t hinge_loss 0.6298\t time 28.8002\n",
      "count 37\t i 4746\t stepsize 0.1920 \t hinge_loss 0.6298\t time 29.5975\n",
      "count 38\t i 5666\t stepsize 0.1906 \t hinge_loss 0.6298\t time 30.4262\n",
      "count 39\t i 25260\t stepsize 0.1892 \t hinge_loss 0.6298\t time 31.2611\n",
      "count 40\t i 7254\t stepsize 0.1879 \t hinge_loss 0.7756\t time 32.1101\n",
      "count 41\t i 3647\t stepsize 0.1867 \t hinge_loss 0.7756\t time 32.9457\n",
      "count 42\t i 3185\t stepsize 0.1854 \t hinge_loss 0.7756\t time 33.7727\n",
      "count 43\t i 3002\t stepsize 0.1843 \t hinge_loss 0.7756\t time 34.5887\n",
      "count 44\t i 2711\t stepsize 0.1832 \t hinge_loss 0.7756\t time 35.4009\n",
      "count 45\t i 22122\t stepsize 0.1821 \t hinge_loss 0.7756\t time 36.2053\n",
      "count 46\t i 10947\t stepsize 0.1810 \t hinge_loss 0.7756\t time 37.0042\n",
      "count 47\t i 12366\t stepsize 0.1800 \t hinge_loss 0.7756\t time 37.8181\n",
      "count 48\t i 20383\t stepsize 0.1791 \t hinge_loss 0.7756\t time 38.6323\n",
      "count 49\t i 5389\t stepsize 0.1781 \t hinge_loss 0.7756\t time 39.4638\n",
      "count 50\t i 21261\t stepsize 0.1772 \t hinge_loss 0.6401\t time 40.2717\n",
      "count 51\t i 25033\t stepsize 0.1763 \t hinge_loss 0.6401\t time 41.1037\n",
      "count 52\t i 25044\t stepsize 0.1754 \t hinge_loss 0.6401\t time 41.9160\n",
      "count 53\t i 13194\t stepsize 0.1746 \t hinge_loss 0.6401\t time 42.7636\n",
      "count 54\t i 22591\t stepsize 0.1738 \t hinge_loss 0.6401\t time 43.5678\n",
      "count 55\t i 14977\t stepsize 0.1730 \t hinge_loss 0.6401\t time 44.3905\n",
      "count 56\t i 168\t stepsize 0.1722 \t hinge_loss 0.6401\t time 45.1939\n",
      "count 57\t i 2043\t stepsize 0.1714 \t hinge_loss 0.6401\t time 46.0300\n",
      "count 58\t i 1484\t stepsize 0.1707 \t hinge_loss 0.6420\t time 46.8474\n",
      "count 59\t i 5415\t stepsize 0.1700 \t hinge_loss 0.6704\t time 47.6516\n",
      "count 60\t i 17273\t stepsize 0.1693 \t hinge_loss 0.8454\t time 48.4796\n",
      "count 61\t i 23437\t stepsize 0.1686 \t hinge_loss 0.6751\t time 49.2838\n",
      "count 62\t i 22802\t stepsize 0.1679 \t hinge_loss 0.6751\t time 50.0662\n",
      "count 63\t i 19969\t stepsize 0.1673 \t hinge_loss 0.6751\t time 50.8957\n",
      "count 64\t i 9440\t stepsize 0.1667 \t hinge_loss 0.6751\t time 51.7368\n",
      "count 65\t i 16916\t stepsize 0.1660 \t hinge_loss 0.6751\t time 52.5711\n",
      "count 66\t i 25474\t stepsize 0.1654 \t hinge_loss 0.6751\t time 53.3997\n",
      "count 67\t i 23199\t stepsize 0.1649 \t hinge_loss 0.6751\t time 54.2388\n",
      "count 68\t i 20013\t stepsize 0.1643 \t hinge_loss 0.6751\t time 55.0654\n",
      "count 69\t i 4928\t stepsize 0.1637 \t hinge_loss 0.6751\t time 55.9260\n",
      "count 70\t i 19437\t stepsize 0.1632 \t hinge_loss 0.6751\t time 56.7391\n",
      "count 71\t i 1056\t stepsize 0.1626 \t hinge_loss 0.6751\t time 57.5718\n",
      "count 72\t i 25708\t stepsize 0.1621 \t hinge_loss 0.6751\t time 58.3883\n",
      "count 73\t i 27446\t stepsize 0.1616 \t hinge_loss 0.6751\t time 59.2175\n",
      "count 74\t i 18240\t stepsize 0.1610 \t hinge_loss 0.6751\t time 60.0503\n",
      "count 75\t i 20416\t stepsize 0.1605 \t hinge_loss 0.7748\t time 60.8910\n",
      "count 76\t i 20864\t stepsize 0.1601 \t hinge_loss 0.7748\t time 61.6919\n",
      "count 77\t i 28626\t stepsize 0.1596 \t hinge_loss 0.7748\t time 62.5122\n",
      "count 78\t i 23795\t stepsize 0.1591 \t hinge_loss 0.7748\t time 63.3305\n",
      "count 79\t i 19513\t stepsize 0.1586 \t hinge_loss 0.7748\t time 64.1950\n",
      "count 80\t i 15796\t stepsize 0.1582 \t hinge_loss 0.9858\t time 64.9847\n",
      "count 81\t i 5049\t stepsize 0.1577 \t hinge_loss 0.9858\t time 65.7947\n",
      "count 82\t i 24199\t stepsize 0.1573 \t hinge_loss 0.7222\t time 66.6507\n",
      "count 83\t i 8185\t stepsize 0.1569 \t hinge_loss 0.7222\t time 67.4829\n",
      "count 84\t i 12314\t stepsize 0.1564 \t hinge_loss 0.6157\t time 68.2750\n",
      "count 85\t i 9615\t stepsize 0.1560 \t hinge_loss 0.6875\t time 69.0957\n",
      "count 86\t i 21490\t stepsize 0.1556 \t hinge_loss 0.6875\t time 69.9083\n",
      "count 87\t i 20565\t stepsize 0.1552 \t hinge_loss 0.6875\t time 70.7306\n",
      "count 88\t i 8549\t stepsize 0.1548 \t hinge_loss 0.6875\t time 71.5465\n",
      "count 89\t i 384\t stepsize 0.1544 \t hinge_loss 0.6059\t time 72.3864\n",
      "count 90\t i 5223\t stepsize 0.1540 \t hinge_loss 0.6059\t time 73.2118\n",
      "count 91\t i 17937\t stepsize 0.1537 \t hinge_loss 0.6059\t time 74.0369\n",
      "count 92\t i 12266\t stepsize 0.1533 \t hinge_loss 0.6059\t time 74.8738\n",
      "count 93\t i 25859\t stepsize 0.1529 \t hinge_loss 0.6632\t time 75.7133\n",
      "count 94\t i 8127\t stepsize 0.1526 \t hinge_loss 0.6632\t time 76.5326\n",
      "count 95\t i 15402\t stepsize 0.1522 \t hinge_loss 0.6632\t time 77.3664\n",
      "count 96\t i 24354\t stepsize 0.1519 \t hinge_loss 0.6632\t time 78.2085\n",
      "count 97\t i 16220\t stepsize 0.1515 \t hinge_loss 0.6632\t time 79.0649\n",
      "count 98\t i 24948\t stepsize 0.1512 \t hinge_loss 0.6632\t time 79.9187\n",
      "count 99\t i 22243\t stepsize 0.1508 \t hinge_loss 0.6632\t time 80.7186\n",
      "count 100\t i 6520\t stepsize 0.1505 \t hinge_loss 0.6632\t time 81.5506\n",
      "count 101\t i 17526\t stepsize 0.1502 \t hinge_loss 0.6632\t time 82.3536\n",
      "count 102\t i 16737\t stepsize 0.1499 \t hinge_loss 0.6632\t time 83.1803\n",
      "count 103\t i 13591\t stepsize 0.1496 \t hinge_loss 0.6632\t time 83.9935\n",
      "count 104\t i 2010\t stepsize 0.1492 \t hinge_loss 0.6632\t time 84.8411\n",
      "count 105\t i 7087\t stepsize 0.1489 \t hinge_loss 0.5969\t time 85.6932\n",
      "count 106\t i 24184\t stepsize 0.1486 \t hinge_loss 0.7240\t time 86.5068\n",
      "count 107\t i 20180\t stepsize 0.1483 \t hinge_loss 0.7240\t time 87.3184\n",
      "count 108\t i 26453\t stepsize 0.1480 \t hinge_loss 0.5996\t time 88.1696\n",
      "count 109\t i 10324\t stepsize 0.1478 \t hinge_loss 0.5996\t time 88.9953\n",
      "count 110\t i 4287\t stepsize 0.1475 \t hinge_loss 0.5996\t time 89.8201\n",
      "count 111\t i 573\t stepsize 0.1472 \t hinge_loss 0.6164\t time 90.6559\n",
      "count 112\t i 11335\t stepsize 0.1469 \t hinge_loss 0.6164\t time 91.4976\n",
      "count 113\t i 17952\t stepsize 0.1466 \t hinge_loss 0.6164\t time 92.3171\n",
      "count 114\t i 24469\t stepsize 0.1464 \t hinge_loss 0.7525\t time 93.1277\n",
      "count 115\t i 18598\t stepsize 0.1461 \t hinge_loss 0.7525\t time 93.9900\n",
      "count 116\t i 24803\t stepsize 0.1458 \t hinge_loss 0.7525\t time 94.8020\n",
      "count 117\t i 1447\t stepsize 0.1456 \t hinge_loss 0.9272\t time 95.6509\n",
      "count 118\t i 2654\t stepsize 0.1453 \t hinge_loss 0.9272\t time 96.5728\n",
      "count 119\t i 12466\t stepsize 0.1450 \t hinge_loss 0.9272\t time 97.4118\n",
      "count 120\t i 16366\t stepsize 0.1448 \t hinge_loss 0.9272\t time 98.2396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 121\t i 13921\t stepsize 0.1445 \t hinge_loss 0.9272\t time 99.0616\n",
      "count 122\t i 26198\t stepsize 0.1443 \t hinge_loss 0.9272\t time 99.8728\n",
      "count 123\t i 15381\t stepsize 0.1440 \t hinge_loss 0.9272\t time 100.6977\n",
      "count 124\t i 17291\t stepsize 0.1438 \t hinge_loss 0.9272\t time 101.5134\n",
      "count 125\t i 28145\t stepsize 0.1436 \t hinge_loss 0.9272\t time 102.2961\n",
      "count 126\t i 8663\t stepsize 0.1433 \t hinge_loss 0.7206\t time 103.1097\n",
      "count 127\t i 18082\t stepsize 0.1431 \t hinge_loss 0.7206\t time 103.9195\n",
      "count 128\t i 24788\t stepsize 0.1429 \t hinge_loss 0.9064\t time 104.7035\n",
      "count 129\t i 11001\t stepsize 0.1426 \t hinge_loss 0.7238\t time 105.5354\n",
      "count 130\t i 14552\t stepsize 0.1424 \t hinge_loss 0.6044\t time 106.3762\n",
      "count 131\t i 20366\t stepsize 0.1422 \t hinge_loss 0.6044\t time 107.1901\n",
      "count 132\t i 2961\t stepsize 0.1420 \t hinge_loss 0.6044\t time 107.9756\n",
      "count 133\t i 15844\t stepsize 0.1417 \t hinge_loss 0.6044\t time 108.8118\n",
      "count 134\t i 6025\t stepsize 0.1415 \t hinge_loss 0.6044\t time 109.6339\n",
      "count 135\t i 28247\t stepsize 0.1413 \t hinge_loss 0.6044\t time 110.4111\n",
      "count 136\t i 5929\t stepsize 0.1411 \t hinge_loss 0.6044\t time 111.2378\n",
      "count 137\t i 15669\t stepsize 0.1409 \t hinge_loss 0.6044\t time 112.0919\n",
      "count 138\t i 11102\t stepsize 0.1407 \t hinge_loss 0.6044\t time 112.8830\n",
      "count 139\t i 2325\t stepsize 0.1405 \t hinge_loss 0.7251\t time 113.6959\n",
      "count 140\t i 10983\t stepsize 0.1403 \t hinge_loss 0.7251\t time 114.5175\n",
      "count 141\t i 12864\t stepsize 0.1401 \t hinge_loss 0.7251\t time 115.3592\n",
      "count 142\t i 25118\t stepsize 0.1399 \t hinge_loss 0.7251\t time 116.2045\n",
      "count 143\t i 27530\t stepsize 0.1397 \t hinge_loss 0.7251\t time 117.0193\n",
      "count 144\t i 18776\t stepsize 0.1395 \t hinge_loss 0.9049\t time 117.8542\n",
      "count 145\t i 3862\t stepsize 0.1393 \t hinge_loss 0.9049\t time 118.6976\n",
      "count 146\t i 18764\t stepsize 0.1391 \t hinge_loss 0.7134\t time 119.5147\n",
      "count 147\t i 28453\t stepsize 0.1389 \t hinge_loss 0.6053\t time 120.3583\n",
      "count 148\t i 21949\t stepsize 0.1387 \t hinge_loss 0.6053\t time 121.1820\n",
      "count 149\t i 21413\t stepsize 0.1385 \t hinge_loss 0.6053\t time 121.9975\n",
      "count 150\t i 5957\t stepsize 0.1383 \t hinge_loss 0.6053\t time 122.7837\n",
      "count 151\t i 13585\t stepsize 0.1382 \t hinge_loss 0.6053\t time 123.5832\n",
      "count 152\t i 4177\t stepsize 0.1380 \t hinge_loss 0.6053\t time 124.3897\n",
      "count 153\t i 20830\t stepsize 0.1378 \t hinge_loss 0.6046\t time 125.2293\n",
      "count 154\t i 8590\t stepsize 0.1376 \t hinge_loss 0.6046\t time 126.0381\n",
      "count 155\t i 19430\t stepsize 0.1374 \t hinge_loss 0.6071\t time 126.8594\n",
      "count 156\t i 10214\t stepsize 0.1373 \t hinge_loss 0.6071\t time 127.6727\n",
      "count 157\t i 15899\t stepsize 0.1371 \t hinge_loss 0.6071\t time 128.4894\n",
      "count 158\t i 24662\t stepsize 0.1369 \t hinge_loss 0.6071\t time 129.2752\n",
      "count 159\t i 18825\t stepsize 0.1367 \t hinge_loss 0.6071\t time 130.0661\n",
      "count 160\t i 7131\t stepsize 0.1366 \t hinge_loss 0.6071\t time 130.8595\n",
      "count 161\t i 13800\t stepsize 0.1364 \t hinge_loss 0.6071\t time 131.6559\n",
      "count 162\t i 29039\t stepsize 0.1362 \t hinge_loss 0.6071\t time 132.5024\n",
      "count 163\t i 22111\t stepsize 0.1361 \t hinge_loss 0.6047\t time 133.3398\n",
      "count 164\t i 7700\t stepsize 0.1359 \t hinge_loss 0.5832\t time 134.1585\n",
      "count 165\t i 4902\t stepsize 0.1358 \t hinge_loss 0.5832\t time 134.9830\n",
      "count 166\t i 26563\t stepsize 0.1356 \t hinge_loss 0.6625\t time 135.8186\n",
      "count 167\t i 8300\t stepsize 0.1354 \t hinge_loss 0.5717\t time 136.6257\n",
      "count 168\t i 12844\t stepsize 0.1353 \t hinge_loss 0.6139\t time 137.4522\n",
      "count 169\t i 10013\t stepsize 0.1351 \t hinge_loss 0.6139\t time 138.2459\n",
      "count 170\t i 18158\t stepsize 0.1350 \t hinge_loss 0.6139\t time 139.0570\n",
      "count 171\t i 7893\t stepsize 0.1348 \t hinge_loss 0.6139\t time 139.8770\n",
      "count 172\t i 25023\t stepsize 0.1347 \t hinge_loss 0.6139\t time 140.7122\n",
      "count 173\t i 12416\t stepsize 0.1345 \t hinge_loss 0.6139\t time 141.5446\n",
      "count 174\t i 5148\t stepsize 0.1344 \t hinge_loss 0.5672\t time 142.3409\n",
      "count 175\t i 24473\t stepsize 0.1342 \t hinge_loss 0.6145\t time 143.1588\n",
      "count 176\t i 2351\t stepsize 0.1341 \t hinge_loss 0.6145\t time 143.9363\n",
      "count 177\t i 12949\t stepsize 0.1339 \t hinge_loss 0.6145\t time 144.7435\n",
      "count 178\t i 15752\t stepsize 0.1338 \t hinge_loss 0.5788\t time 145.5869\n",
      "count 179\t i 9256\t stepsize 0.1336 \t hinge_loss 0.5788\t time 146.3989\n",
      "count 180\t i 2852\t stepsize 0.1335 \t hinge_loss 0.5788\t time 147.2193\n",
      "count 181\t i 24757\t stepsize 0.1333 \t hinge_loss 0.5788\t time 148.0289\n",
      "count 182\t i 2623\t stepsize 0.1332 \t hinge_loss 0.5788\t time 148.8566\n",
      "count 183\t i 17124\t stepsize 0.1331 \t hinge_loss 0.5788\t time 149.6973\n",
      "count 184\t i 1357\t stepsize 0.1329 \t hinge_loss 0.5788\t time 150.4829\n",
      "count 185\t i 8721\t stepsize 0.1328 \t hinge_loss 0.5788\t time 151.3120\n",
      "count 186\t i 13717\t stepsize 0.1326 \t hinge_loss 0.5788\t time 152.1347\n",
      "count 187\t i 26546\t stepsize 0.1325 \t hinge_loss 0.5788\t time 152.9343\n",
      "count 188\t i 7568\t stepsize 0.1324 \t hinge_loss 0.5788\t time 153.7466\n",
      "count 189\t i 16569\t stepsize 0.1322 \t hinge_loss 0.6459\t time 154.5530\n",
      "count 190\t i 28127\t stepsize 0.1321 \t hinge_loss 0.6459\t time 155.3624\n",
      "count 191\t i 1213\t stepsize 0.1320 \t hinge_loss 0.7518\t time 156.1470\n",
      "count 192\t i 18313\t stepsize 0.1318 \t hinge_loss 0.6085\t time 156.9464\n",
      "count 193\t i 29142\t stepsize 0.1317 \t hinge_loss 0.5567\t time 157.7765\n",
      "count 194\t i 17663\t stepsize 0.1316 \t hinge_loss 0.5803\t time 158.5884\n",
      "count 195\t i 14760\t stepsize 0.1315 \t hinge_loss 0.5471\t time 159.3805\n",
      "count 196\t i 29183\t stepsize 0.1313 \t hinge_loss 0.6637\t time 160.2053\n",
      "count 197\t i 1141\t stepsize 0.1312 \t hinge_loss 0.6637\t time 161.0464\n",
      "count 198\t i 17416\t stepsize 0.1311 \t hinge_loss 0.6637\t time 161.8904\n",
      "count 199\t i 10165\t stepsize 0.1309 \t hinge_loss 0.6637\t time 162.9845\n",
      "count 200\t i 20688\t stepsize 0.1308 \t hinge_loss 0.5615\t time 164.0013\n",
      "count 201\t i 7693\t stepsize 0.1307 \t hinge_loss 0.6705\t time 165.0724\n",
      "count 202\t i 13831\t stepsize 0.1306 \t hinge_loss 0.6705\t time 166.0394\n",
      "count 203\t i 21443\t stepsize 0.1305 \t hinge_loss 0.6705\t time 167.1502\n",
      "count 204\t i 23974\t stepsize 0.1303 \t hinge_loss 0.6705\t time 168.1586\n",
      "count 205\t i 9767\t stepsize 0.1302 \t hinge_loss 0.5505\t time 168.9750\n",
      "count 206\t i 220\t stepsize 0.1301 \t hinge_loss 0.5505\t time 169.7722\n",
      "count 207\t i 24851\t stepsize 0.1300 \t hinge_loss 0.5505\t time 170.6022\n",
      "count 208\t i 10605\t stepsize 0.1299 \t hinge_loss 0.5505\t time 171.4333\n",
      "count 209\t i 24863\t stepsize 0.1297 \t hinge_loss 0.5505\t time 172.2474\n",
      "count 210\t i 20448\t stepsize 0.1296 \t hinge_loss 0.6767\t time 173.0548\n",
      "count 211\t i 23204\t stepsize 0.1295 \t hinge_loss 0.6767\t time 173.8891\n",
      "count 212\t i 12153\t stepsize 0.1294 \t hinge_loss 0.6767\t time 174.6874\n",
      "count 213\t i 18711\t stepsize 0.1293 \t hinge_loss 0.6767\t time 175.4929\n",
      "count 214\t i 21785\t stepsize 0.1292 \t hinge_loss 0.6767\t time 176.4032\n",
      "count 215\t i 20034\t stepsize 0.1291 \t hinge_loss 0.5792\t time 177.3668\n",
      "count 216\t i 26076\t stepsize 0.1290 \t hinge_loss 0.5792\t time 178.2662\n",
      "count 217\t i 9610\t stepsize 0.1288 \t hinge_loss 0.5479\t time 179.1018\n",
      "count 218\t i 8661\t stepsize 0.1287 \t hinge_loss 0.5479\t time 179.9040\n",
      "count 219\t i 24261\t stepsize 0.1286 \t hinge_loss 0.5479\t time 180.7436\n",
      "count 220\t i 16852\t stepsize 0.1285 \t hinge_loss 0.5469\t time 181.5836\n",
      "count 221\t i 24659\t stepsize 0.1284 \t hinge_loss 0.5406\t time 182.4110\n",
      "count 222\t i 3389\t stepsize 0.1283 \t hinge_loss 0.6248\t time 183.2520\n",
      "count 223\t i 19983\t stepsize 0.1282 \t hinge_loss 0.6248\t time 184.3486\n",
      "count 224\t i 4817\t stepsize 0.1281 \t hinge_loss 0.5417\t time 185.2775\n",
      "count 225\t i 7822\t stepsize 0.1280 \t hinge_loss 0.5519\t time 186.2234\n",
      "count 226\t i 6084\t stepsize 0.1279 \t hinge_loss 0.5476\t time 187.1115\n",
      "count 227\t i 20933\t stepsize 0.1278 \t hinge_loss 0.5476\t time 188.0712\n",
      "count 228\t i 15307\t stepsize 0.1277 \t hinge_loss 0.5570\t time 188.9246\n",
      "count 229\t i 8849\t stepsize 0.1276 \t hinge_loss 0.5570\t time 189.7641\n",
      "count 230\t i 19106\t stepsize 0.1275 \t hinge_loss 0.5570\t time 190.8295\n",
      "count 231\t i 10808\t stepsize 0.1274 \t hinge_loss 0.5570\t time 191.9266\n",
      "count 232\t i 10179\t stepsize 0.1273 \t hinge_loss 0.5570\t time 192.7946\n",
      "count 233\t i 20179\t stepsize 0.1272 \t hinge_loss 0.7243\t time 193.7644\n",
      "count 234\t i 11489\t stepsize 0.1271 \t hinge_loss 0.5842\t time 194.6844\n",
      "count 235\t i 4276\t stepsize 0.1270 \t hinge_loss 0.5842\t time 195.5288\n",
      "count 236\t i 4439\t stepsize 0.1269 \t hinge_loss 0.7442\t time 196.3937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 237\t i 14501\t stepsize 0.1268 \t hinge_loss 0.5700\t time 197.2584\n",
      "count 238\t i 11597\t stepsize 0.1267 \t hinge_loss 0.5390\t time 198.1267\n",
      "count 239\t i 1675\t stepsize 0.1266 \t hinge_loss 0.5390\t time 198.9258\n",
      "count 240\t i 24360\t stepsize 0.1265 \t hinge_loss 0.5390\t time 199.7585\n",
      "count 241\t i 10657\t stepsize 0.1264 \t hinge_loss 0.5802\t time 200.6393\n",
      "count 242\t i 14573\t stepsize 0.1263 \t hinge_loss 0.5802\t time 201.4878\n",
      "count 243\t i 12994\t stepsize 0.1262 \t hinge_loss 0.5294\t time 202.3154\n",
      "count 244\t i 27453\t stepsize 0.1261 \t hinge_loss 0.5982\t time 203.1717\n",
      "count 245\t i 14951\t stepsize 0.1260 \t hinge_loss 0.5982\t time 204.0012\n",
      "count 246\t i 13399\t stepsize 0.1259 \t hinge_loss 0.5311\t time 204.8046\n",
      "count 247\t i 4945\t stepsize 0.1258 \t hinge_loss 0.5311\t time 205.6186\n",
      "count 248\t i 767\t stepsize 0.1257 \t hinge_loss 0.5311\t time 206.4336\n",
      "count 249\t i 19125\t stepsize 0.1256 \t hinge_loss 0.5311\t time 207.2492\n",
      "count 250\t i 25888\t stepsize 0.1255 \t hinge_loss 0.6177\t time 208.0407\n",
      "count 251\t i 14132\t stepsize 0.1254 \t hinge_loss 0.6177\t time 208.8874\n",
      "count 252\t i 10320\t stepsize 0.1254 \t hinge_loss 0.6177\t time 209.6737\n",
      "count 253\t i 11596\t stepsize 0.1253 \t hinge_loss 0.6177\t time 210.5041\n",
      "count 254\t i 28778\t stepsize 0.1252 \t hinge_loss 0.6177\t time 211.3286\n",
      "count 255\t i 22455\t stepsize 0.1251 \t hinge_loss 0.6177\t time 212.2811\n",
      "count 256\t i 26335\t stepsize 0.1250 \t hinge_loss 0.7677\t time 213.1979\n",
      "count 257\t i 27892\t stepsize 0.1249 \t hinge_loss 0.6045\t time 214.1479\n",
      "count 258\t i 27832\t stepsize 0.1248 \t hinge_loss 0.6045\t time 215.1602\n",
      "count 259\t i 14730\t stepsize 0.1247 \t hinge_loss 0.6045\t time 216.1274\n",
      "count 260\t i 18377\t stepsize 0.1247 \t hinge_loss 0.5187\t time 217.1752\n",
      "count 261\t i 3746\t stepsize 0.1246 \t hinge_loss 0.5187\t time 218.0639\n",
      "count 262\t i 8639\t stepsize 0.1245 \t hinge_loss 0.5187\t time 218.9659\n",
      "count 263\t i 16079\t stepsize 0.1244 \t hinge_loss 0.5187\t time 219.8586\n",
      "count 264\t i 6144\t stepsize 0.1243 \t hinge_loss 0.5187\t time 220.8049\n",
      "count 265\t i 17171\t stepsize 0.1242 \t hinge_loss 0.5187\t time 221.6690\n",
      "count 266\t i 14056\t stepsize 0.1241 \t hinge_loss 0.5187\t time 222.6666\n",
      "count 267\t i 5507\t stepsize 0.1241 \t hinge_loss 0.5187\t time 223.6229\n",
      "count 268\t i 14501\t stepsize 0.1240 \t hinge_loss 0.5926\t time 225.2267\n",
      "count 269\t i 6572\t stepsize 0.1239 \t hinge_loss 0.5926\t time 226.3728\n",
      "count 270\t i 515\t stepsize 0.1238 \t hinge_loss 0.5926\t time 227.3014\n",
      "count 271\t i 26662\t stepsize 0.1237 \t hinge_loss 0.5926\t time 228.1616\n",
      "count 272\t i 15679\t stepsize 0.1236 \t hinge_loss 0.7515\t time 228.9968\n",
      "count 273\t i 10606\t stepsize 0.1236 \t hinge_loss 0.7515\t time 229.8138\n",
      "count 274\t i 14531\t stepsize 0.1235 \t hinge_loss 0.5909\t time 230.6153\n",
      "count 275\t i 18082\t stepsize 0.1234 \t hinge_loss 0.5909\t time 231.4460\n",
      "count 276\t i 28242\t stepsize 0.1233 \t hinge_loss 0.7404\t time 232.2540\n",
      "count 277\t i 6440\t stepsize 0.1232 \t hinge_loss 0.7404\t time 233.1038\n",
      "count 278\t i 28042\t stepsize 0.1232 \t hinge_loss 0.7404\t time 233.9550\n",
      "count 279\t i 18978\t stepsize 0.1231 \t hinge_loss 0.7404\t time 234.8723\n",
      "count 280\t i 12458\t stepsize 0.1230 \t hinge_loss 0.7404\t time 235.7251\n",
      "count 281\t i 24377\t stepsize 0.1229 \t hinge_loss 0.7404\t time 236.6645\n",
      "count 282\t i 12843\t stepsize 0.1229 \t hinge_loss 0.7404\t time 237.9905\n",
      "count 283\t i 23233\t stepsize 0.1228 \t hinge_loss 0.5779\t time 238.8776\n",
      "count 284\t i 25128\t stepsize 0.1227 \t hinge_loss 0.5779\t time 239.6739\n",
      "count 285\t i 25097\t stepsize 0.1226 \t hinge_loss 0.5779\t time 240.5012\n",
      "count 286\t i 10420\t stepsize 0.1226 \t hinge_loss 0.6742\t time 241.3863\n",
      "count 287\t i 17058\t stepsize 0.1225 \t hinge_loss 0.6742\t time 242.1984\n",
      "count 288\t i 22330\t stepsize 0.1224 \t hinge_loss 0.6742\t time 243.0072\n",
      "count 289\t i 7712\t stepsize 0.1223 \t hinge_loss 0.5559\t time 243.8791\n",
      "count 290\t i 6337\t stepsize 0.1223 \t hinge_loss 0.5559\t time 244.8741\n",
      "count 291\t i 15709\t stepsize 0.1222 \t hinge_loss 0.5559\t time 245.8045\n",
      "count 292\t i 18906\t stepsize 0.1221 \t hinge_loss 0.5559\t time 246.7700\n",
      "count 293\t i 20922\t stepsize 0.1220 \t hinge_loss 0.5559\t time 248.6679\n",
      "count 294\t i 14021\t stepsize 0.1220 \t hinge_loss 0.5559\t time 250.2852\n",
      "count 295\t i 21974\t stepsize 0.1219 \t hinge_loss 0.5559\t time 251.3412\n",
      "count 296\t i 23857\t stepsize 0.1218 \t hinge_loss 0.5559\t time 252.3646\n",
      "count 297\t i 19845\t stepsize 0.1217 \t hinge_loss 0.5040\t time 253.7056\n",
      "count 298\t i 17029\t stepsize 0.1217 \t hinge_loss 0.5325\t time 254.9398\n",
      "count 299\t i 9429\t stepsize 0.1216 \t hinge_loss 0.5201\t time 255.9071\n",
      "count 300\t i 24460\t stepsize 0.1215 \t hinge_loss 0.5201\t time 257.0435\n",
      "count 301\t i 13310\t stepsize 0.1215 \t hinge_loss 0.5201\t time 259.1741\n",
      "count 302\t i 5021\t stepsize 0.1214 \t hinge_loss 0.5201\t time 260.2852\n",
      "count 303\t i 9432\t stepsize 0.1213 \t hinge_loss 0.5201\t time 261.2151\n",
      "count 304\t i 7976\t stepsize 0.1212 \t hinge_loss 0.6075\t time 262.0184\n",
      "count 305\t i 9461\t stepsize 0.1212 \t hinge_loss 0.6075\t time 262.8788\n",
      "count 306\t i 9608\t stepsize 0.1211 \t hinge_loss 0.6075\t time 263.7204\n",
      "count 307\t i 15690\t stepsize 0.1210 \t hinge_loss 0.5069\t time 264.5724\n",
      "count 308\t i 108\t stepsize 0.1210 \t hinge_loss 0.5477\t time 265.3972\n",
      "count 309\t i 3821\t stepsize 0.1209 \t hinge_loss 0.5116\t time 266.2807\n",
      "count 310\t i 25225\t stepsize 0.1208 \t hinge_loss 0.5427\t time 267.7049\n",
      "count 311\t i 1277\t stepsize 0.1208 \t hinge_loss 0.5427\t time 268.6851\n",
      "count 312\t i 2691\t stepsize 0.1207 \t hinge_loss 0.6278\t time 269.6050\n",
      "count 313\t i 19213\t stepsize 0.1206 \t hinge_loss 0.6278\t time 270.5524\n",
      "count 314\t i 23517\t stepsize 0.1206 \t hinge_loss 0.5488\t time 271.4707\n",
      "count 315\t i 7780\t stepsize 0.1205 \t hinge_loss 0.4931\t time 272.3590\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Q2 \n",
    "########################################################################\n",
    "# 1) Stochastic sub-gradient\n",
    "def get_hinge_loss(A,x,b):\n",
    "#     print('get_hinge_loss')\n",
    "    A_dot_x = A.dot(x)\n",
    "    ei = A_dot_x.multiply(b).tocsr()  # scipy.sparse.coo.coo_matrix ->csr a matrix \n",
    "    one_error = lambda ei : max(0,1-ei)\n",
    "    total_error = sum(map(one_error,[ei[i,0] for i in range(ei.shape[0])]))\n",
    "    return total_error/A.shape[0]\n",
    "\n",
    "def get_gradient_sto_subgrad(A,x,b,i):\n",
    "#     print('get_gradient')\n",
    "    if 1 - (b[i]*(A[i].dot(x))).tocsr()[0,0] > 0:\n",
    "        grad = (-b[i]*A[i]).reshape(features+1,1).tocsr()\n",
    "        return grad\n",
    "    else: \n",
    "        grad = csr_matrix(np.zeros(shape=(features+1,1), dtype=float))\n",
    "        return grad\n",
    "    \n",
    "def sto_subgrad(A,x,b,max_iteration,epsilon,record_loss):\n",
    "    count = 2\n",
    "    print('stochastic_subgradient, question2',)\n",
    "    loss_list = []\n",
    "    time_list = []\n",
    "    st = time.time()\n",
    "    print(st)\n",
    "    loss = float('inf') # initial a size to start \n",
    "    while count < max_iteration and loss >= epsilon:\n",
    "        index = random.randint(0,b.shape[0])\n",
    "        loss = get_hinge_loss(A,x,b)\n",
    "        grad = get_gradient_sto_subgrad(A,x,b,index)\n",
    "        step_size = 1/math.log(count,2) \n",
    "        \n",
    "        stepsize = 1/math.log(count)\n",
    "        x = x - step_size*grad\n",
    "        time_diff = time.time()-st\n",
    "        print('count %d\\t i %d\\t stepsize %.4f \\t hinge_loss %.4f\\t time %.4f'%(count,index, step_size,loss,time_diff))\n",
    "        count +=1\n",
    "        if record_loss:\n",
    "            loss_list.append(loss)\n",
    "            time_list.append(time_diff)\n",
    "    if record_loss:\n",
    "        return x,count,loss_list,time_list\n",
    "    return x,count,hinge_loss\n",
    "x_ssg,count_ssg,loss_ssg, time_ssg= stochastic_subgradient_Q2(A_training,x_,b_training, 1000,0.5,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "plt.plot(time_ssg,loss_ssg, label=(\"stochastic subgradient\"), linewidth=2.0, color =\"green\")\n",
    "plt.xlabel(\"time(s)\", fontsize=25)\n",
    "plt.ylabel(\"Objective function\", fontsize=25)\n",
    "plt.savefig('stochastic_subgradient.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stochastic_subgradient, question2\n"
     ]
    }
   ],
   "source": [
    "# 1) Stochastic sub-gradient\n",
    "import random\n",
    "\n",
    "lambda_ = 1\n",
    "def get_regression_loss(A,x,b):\n",
    "    bs = hstack([b]*A.shape[1])\n",
    "    t = -1*b.multiply((A.dot(x)))  #-bi(ai.x +b)\n",
    "    # exp(t), element wise\n",
    "    e=t.expm1()\n",
    "    # log(1+exp(t)), element wise\n",
    "    deno = (csr_matrix(np.ones(shape=(b.shape[0],b.shape[1])),shape=(b.shape[0],b.shape[1])) + e).log1p()\n",
    "    loss = lambda_* norm(x.toarray(),2)**2 + deno.mean(0)[0,0]\n",
    "    return loss\n",
    "\n",
    "# get_regression_loss(A_training,x_,b_training)\n",
    "\n",
    "def get_gradient_sto_grad(A,x,b):\n",
    "    bs = hstack([b]*A.shape[1])\n",
    "    t = -1*b.multiply((A.dot(x)))  #-bi(ai.x +b)\n",
    "    # exp(t), element wise\n",
    "    nume=t.expm1()\n",
    "    # log(1+exp(t)), element wise\n",
    "    deno = (csr_matrix(np.ones(shape=(b.shape[0],b.shape[1])),shape=(b.shape[0],b.shape[1])) + nume)\n",
    "    coef = csr_matrix(nume/deno)\n",
    "    coefs = hstack([coef]*b.shape[1])\n",
    "\n",
    "    _biai = -1*bs.multiply(A)\n",
    "    g = _biai.multiply(coefs)\n",
    "    # next, need to sum by column and divide by n\n",
    "    grad = csr_matrix(g.mean(0)).transpose()\n",
    "    print(grad)\n",
    "    return grad\n",
    "    \n",
    "    \n",
    "# print(get_gradient_sto_grad(A_training,x_,b_training))\n",
    "    \n",
    "def stochastic_gradient(A,x,b,max_iteration,epsilon,record_loss=False):\n",
    "    count = 1000\n",
    "    print('stochastic_subgradient, question2')\n",
    "    loss_list = []\n",
    "    time_list = []\n",
    "    st = time.time()\n",
    "    loss = epsilon # initial a size to start \n",
    "    while count < max_iteration and loss >= epsilon:\n",
    "        loss = get_regression_loss(A,x,b)\n",
    "        grad = get_gradient_sto_grad(A,x,b)\n",
    "#         step_size = 1/math.log(count,2) \n",
    "        step_size = 1/count\n",
    "        time_diff = time.time()-st\n",
    "        x = x - step_size*grad\n",
    "        print('count %d\\t stepsize %.4f \\t time %.4f \\t hinge_loss %.4f'%(count, step_size, time_diff,loss))\n",
    "        count +=1\n",
    "        if record_loss:\n",
    "            loss_list.append(loss)\n",
    "            time_list.append(time_diff)\n",
    "    if record_loss:\n",
    "        return x,count,loss_list,time_list\n",
    "    return x,count,hinge_loss\n",
    "\n",
    "\n",
    "x_s,count_sgd,loss_sgd,time_sgd= stochastic_gradient(A_training,x_,b_training, 1000,0.5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_gradient(A_,x_,b_,10).toarray()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29304, 124) (124, 1) (29304, 1)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> <class 'numpy.ndarray'> <class 'scipy.sparse.csr.csr_matrix'> <class 'numpy.ndarray'>\n",
      "(29304, 124) (29304, 1) (124, 1) (29304, 1)\n",
      "(29304, 1) (1, 124)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "def gd(A,x,b):\n",
    "    b = b.toarray()\n",
    "#     x = np.array(x)\n",
    "    A_dot_x = (A.dot(x).toarray())\n",
    "    print(type(A),type(b),type(x),type(A_dot_x))\n",
    "    print(A.shape,b.shape,x.shape,A_dot_x.shape)\n",
    "    e = np.exp(-1*A_dot_x*b)\n",
    "    coef = e/(1+e)\n",
    "    biai = -1*A.multiply(b).mean(0)\n",
    "    print(coef.shape,biai.shape)\n",
    "    \n",
    "    \n",
    "    print('finish')\n",
    "\n",
    "    \n",
    "#     t = np.exp(-1*np.array(A.dot(x)) * b)\n",
    "#     print(t.shape)\n",
    "#     # exp(t), element wise\n",
    "#     nume=t.expm1()\n",
    "#     # log(1+exp(t)), element wise\n",
    "#     deno = (csr_matrix(np.ones(shape=(b.shape[0],b.shape[1])),shape=(b.shape[0],b.shape[1])) + nume)\n",
    "#     coef = csr_matrix(nume/deno)\n",
    "#     coefs = hstack([coef]*b.shape[1])\n",
    "\n",
    "#     _biai = -1*bs.multiply(A)\n",
    "#     g = _biai.multiply(coefs)\n",
    "#     # next, need to sum by column and divide by n\n",
    "#     grad = csr_matrix(g.mean(0)).transpose()\n",
    "#     print(grad)\n",
    "#     return grad\n",
    "    \n",
    "print(A_training.shape, x_.shape, b_training.shape)\n",
    "gd(A_training,x_,b_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
