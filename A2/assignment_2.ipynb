{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 2\n",
    "\n",
    "## The assignment is divided into programming and mathematical questions. Both of them are given in this notebook.\n",
    "\n",
    "## Programming questions: I am giving you a template that you can use to write your code. Description of the questions is integrated in the comments.\n",
    "\n",
    "## Upload your code on Learn dropbox and submit pdfs of the code and answers to the mathematical questions on Crowdmark.\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy, scipy, scikit-image, skimage, matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Numpy is useful for handling arrays and matrices.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data.astronaut()\n",
    "img = rgb2gray(img)*255 # convert to gray and change scale from (0,1) to (0,255).\n",
    "\n",
    "n = img.shape[0]\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the differences operators here. Use your code from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need these three methods to construct sparse differences operators.\n",
    "# If you do not use sparse operators you might have scalability problems.\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import kron\n",
    "from scipy.sparse import identity\n",
    "\n",
    "# Use your code from Assignment 1. \n",
    "# Make sure that you compute the right D_h and D_v matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = 0\n",
    "standard_deviation = 30\n",
    "dimensions = (n,n)\n",
    "\n",
    "noise = np.random.normal(mean_,standard_deviation,dimensions)\n",
    "\n",
    "noisy_image = img + noise\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.imshow(noisy_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: implement gradient descent using the Lipschitz constant as the step-size for the denoising problem. Use eigsh method from scipy.sparse.linalg to compute the Lipschitz constant. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, epsilon, lambda_, max_iterations):\n",
    "    \n",
    "# x0: is the initial guess for the x variables\n",
    "# epsilon: is the termination tolerance parameter\n",
    "# lambda_: is the regularization parameter of the denoising problem.\n",
    "# max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent.\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rellative objective function vs number of iterations. \n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "# Here is an example code \n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(store_data_for_plotting, label=(\"Gradient descent + Lipschitz\"), linewidth=5.0, color =\"black\")\n",
    "\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Rellative distance to opt.\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: is there a \"gap\" between the practical convergence rate and the theoretical convergence rate? Note that the denoising objective function is strongly convex. Marks: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: implement gradient descent with line-search for the denoising problem. Marks: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a line-search function here. \n",
    "# I am giving you a hint about what the input could be, but feel free to change the template. \n",
    "def line_search(x,D,Dx,vec_image,grad_reg,grad_fit,lambda_):\n",
    "\n",
    "# D: represents the complex forward differences matrix from the Lecture notes.\n",
    "# Dx: represents the matrix-vector product of D times x.\n",
    "# vec_image: is the vectorized noisy image\n",
    "# grad_reg: is the gradient of the regularization term ||Dx||_2^2\n",
    "# grad_fit: is the gradient of the least-squares term ||x-vec_image||_2^2\n",
    "# lambda_: is the regularization parameter of the denoising problem.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "# Write gradient descent + line-search here.\n",
    "# I am giving you a hint about what the input could be, but feel free to change the template. \n",
    "def gradient_descent(x0, epsilon, lambda_, max_iterations):\n",
    "\n",
    "# x0: is the initial guess for the x variables\n",
    "# epsilon: is the termination tolerance parameter\n",
    "# lambda_: is the regularization parameter of the denoising problem.\n",
    "# max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with line-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rellative objective function vs number of iterations. \n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "# Here is an example code \n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(store_data_for_plotting, label=(\"Gradient descent\"), linewidth=5.0, color =\"red\")\n",
    "\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Rellative distance to opt.\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: What is the advantage of using line-search to compute the step-size at each iteration instead of using constant step-sizes equal to 1/L? Where L is the Lipschitz constant. Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time? Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of running time when you add computation of the Lipschitz constant in the running time? Is gradient descent with line-search faster than gradient descent with constant step-sizes in terms of number of required iterations? Marks: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 5: implement gradient descent with Armijo line-search for the denoising problem. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line-search function\n",
    "def line_search(x,D,Dx,vec_image,grad_reg,grad_fit,lambda_,gamma):\n",
    "\n",
    "# D: represents the complex forward differences matrix from the Lecture notes.\n",
    "# Dx: represents the matrix-vector product of D times x.\n",
    "# vec_image: is the vectorized noisy image\n",
    "# grad_reg: is the gradient of the regularization term ||Dx||_2^2\n",
    "# grad_fit: is the gradient of the least-squares term ||x-vec_image||_2^2\n",
    "# lambda_: is the regularization parameter of the denoising problem.\n",
    "# gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "def gradient_descent(x0, epsilon, lambda_, max_iterations,gamma):\n",
    "\n",
    "# x0: is the initial guess for the x variables\n",
    "# epsilon: is the termination tolerance parameter\n",
    "# lambda_: is the regularization parameter of the denoising problem.\n",
    "# max_iterations: is the maximum number of iterations that you allow the algorithm to run.\n",
    "# gamma: parameter of Armijo line-search as was defined in the lectures.\n",
    "\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Gradient Descent with Armijo line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters of gradient descent\n",
    "lambda_ = 4\n",
    "epsilon = 1.0e-2\n",
    "max_iterations = 2000\n",
    "\n",
    "# Set x0 equal to the vectorized noisy image.\n",
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot $$(f(x_k) - f(x^*)) / (f(x_0) - f(x^*))$$ vs the iteration counter k, where $$x^*$$ is the minimizer of the denoising problem, which you can compute by using spsolve, similarly to Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rellative objective function vs number of iterations. \n",
    "\n",
    "# Write your code here.\n",
    "\n",
    "# Here is an example code \n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.plot(store_data_for_plotting, label=(\"Gradient descent + Armijo\"), linewidth=5.0, color =\"blue\")\n",
    "\n",
    "plt.legend(prop={'size': 20},loc=\"upper right\")\n",
    "plt.xlabel(\"iteration $k$\", fontsize=25)\n",
    "plt.ylabel(\"Objective function\", fontsize=25)\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of running time? Is gradient descent with Armijo line-search faster than gradient descent with simple line-search in terms of number of required iterations? Explain any performance differences between the two approaches. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: prove that the denoising objective function is strongly convex. What is its strong convexity parameter? Marks: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 8: Prove that Armijo line-search will terminate after a finite number of steps. Hint: show that there exists a step-size $$\\alpha^*>0$$ such that for any step-size smaller than $$\\alpha^*$$ the termination condition of Armijo line-search is satisfied. How many iterations will be required in worst-case for Armijo line-search to terminate? Marks 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: what is the running time for gradient descent with Armijo line-search for the denoising problem to achieve $$f(x_k) - f^* \\le \\epsilon$$ ?. The running time is computed by multiplying the worst-case iteration complexity times the FLOPS at each iteration. The FLOPS at each iteration is the number of additions, subtractions, multiplications and divisions that are performed during the current iteration. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Question 10: prove the convergence rate and iteration complexity for gradient descent with constant step-sizes (equal to 1/L) for strongly convex functions. Marks: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
